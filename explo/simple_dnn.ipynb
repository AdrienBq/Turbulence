{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path(sys.path[0]).parent)\n",
    "import modules.utils as utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.9804707 , 14.99043066, 15.00086523, 15.00930762,  0.50063248,\n",
       "        0.50320731,  0.50840387,  0.51110868, -0.12203034, -0.12493407,\n",
       "       -0.12792623, -0.13077889,  0.97752755,  0.97752575,  0.97752387,\n",
       "        0.97752264,  0.13710223,  0.135585  ,  0.13606116,  0.13534779,\n",
       "       -0.11928583, -0.1221244 , -0.12504954, -0.12783829])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 32\n",
    "Directory = f\"data/L_{L}\"\n",
    "\n",
    "variables=['u', 'v', 'w', 'theta', 's', 'tke', 'wtheta']\n",
    "nz=376\n",
    "\n",
    "len_samples = nz*len(variables)\n",
    "len_in = nz*(len(variables)-1)\n",
    "len_out = nz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train test ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [i for i in range(1,64)]\n",
    "perm = np.random.permutation(times)\n",
    "train_times = perm[:int(0.8*63)]\n",
    "test_times = perm[int(0.8*63):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init train ds\n",
    "path_data = Directory+f'/input_ds_for_simple_nn_T{train_times[0]}_L_{L}.nc'\n",
    "ds_init = xr.open_dataset(path_data)\n",
    "df_init = ds_init.to_dataframe()\n",
    "train_ds = df_init.to_numpy()\n",
    "n_samples = len(train_ds)//len_samples\n",
    "train_ds = train_ds.reshape(n_samples, len_samples)\n",
    "\n",
    "for t in train_times[1:]:\n",
    "    path_data = Directory+f'/input_ds_for_simple_nn_T{t}_L_{L}.nc'\n",
    "    ds_init = xr.open_dataset(path_data)\n",
    "    df_init = ds_init.to_dataframe()\n",
    "\n",
    "    time_ds = df_init.to_numpy()\n",
    "    n_samples = len(time_ds)//len_samples\n",
    "    time_ds = time_ds.reshape(n_samples, len_samples)\n",
    "\n",
    "    train_ds = np.concatenate((train_ds, time_ds), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init test ds\n",
    "path_data = Directory+f'/input_ds_for_simple_nn_T{test_times[0]}_L_{L}.nc'\n",
    "ds_init = xr.open_dataset(path_data)\n",
    "df_init = ds_init.to_dataframe()\n",
    "test_ds = df_init.to_numpy()\n",
    "n_samples = len(test_ds)//len_samples\n",
    "test_ds = test_ds.reshape(n_samples, len_samples)\n",
    "\n",
    "for t in test_times[1:]:\n",
    "    path_data = Directory+f'/input_ds_for_simple_nn_T{t}_L_{L}.nc'\n",
    "    ds_init = xr.open_dataset(path_data)\n",
    "    df_init = ds_init.to_dataframe()\n",
    "\n",
    "    time_ds = df_init.to_numpy()\n",
    "    n_samples = len(time_ds)//len_samples\n",
    "    time_ds = time_ds.reshape(n_samples, len_samples)\n",
    "\n",
    "    test_ds = np.concatenate((test_ds, time_ds), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#useless here\n",
    "# split input_ds and output_ds into training and validation sets\n",
    "batch_size = 32\n",
    "train, test = utils.split_train_val(tot_ds,batch_size)\n",
    "input_train, output_train, input_val, output_val = train[:,:len_in], train[:,len_in:], test[:,:len_in], test[:,len_in:]\n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, batch_size, input_size, output_size, drop_prob1=0.5, drop_prob2=0.5, drop_prob3=0.5, hidden_size1=128, hidden_size2=256, hidden_size3=128):\n",
    "        super(DNN, self).__init__()\n",
    "        self.regression = nn.Sequential(nn.Linear(input_size, hidden_size1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(drop_prob1),\n",
    "                                        nn.Linear(hidden_size1, hidden_size2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(drop_prob2),\n",
    "                                        nn.Linear(hidden_size2, hidden_size3),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(drop_prob3),\n",
    "                                        nn.Linear(hidden_size3, output_size)\n",
    "                                        )\n",
    "        self.drop_prob1 = drop_prob1\n",
    "        self.drop_prob2 = drop_prob2\n",
    "        self.drop_prob3 = drop_prob3\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_size\n",
    "        self.output_shape = output_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.regression(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "nb_epochs = 100\n",
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(batch_size=batch_size,input_size=len_in,output_size=len_out)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 3.898409\n",
      "Epoch [20/1000], Loss: 3.299969\n",
      "Epoch [30/1000], Loss: 3.356835\n",
      "Epoch [40/1000], Loss: 3.262183\n",
      "Epoch [50/1000], Loss: 3.168295\n",
      "Epoch [60/1000], Loss: 2.997622\n",
      "Epoch [70/1000], Loss: 2.935409\n",
      "Epoch [80/1000], Loss: 2.178135\n",
      "Epoch [90/1000], Loss: 1.876122\n",
      "Epoch [100/1000], Loss: 1.394014\n",
      "Epoch [110/1000], Loss: 1.059250\n",
      "Epoch [120/1000], Loss: 1.078115\n",
      "Epoch [130/1000], Loss: 0.920705\n",
      "Epoch [140/1000], Loss: 0.740047\n",
      "Epoch [150/1000], Loss: 0.677146\n",
      "Epoch [160/1000], Loss: 0.638483\n",
      "Epoch [170/1000], Loss: 0.653979\n",
      "Epoch [180/1000], Loss: 0.663465\n",
      "Epoch [190/1000], Loss: 0.691787\n",
      "Epoch [200/1000], Loss: 0.442267\n",
      "Epoch [210/1000], Loss: 0.512786\n",
      "Epoch [220/1000], Loss: 0.535509\n",
      "Epoch [230/1000], Loss: 0.425400\n",
      "Epoch [240/1000], Loss: 0.407320\n",
      "Epoch [250/1000], Loss: 0.374976\n",
      "Epoch [260/1000], Loss: 0.374877\n",
      "Epoch [270/1000], Loss: 0.388647\n",
      "Epoch [280/1000], Loss: 0.322622\n",
      "Epoch [290/1000], Loss: 0.360882\n",
      "Epoch [300/1000], Loss: 0.258994\n",
      "Epoch [310/1000], Loss: 0.239691\n",
      "Epoch [320/1000], Loss: 0.281604\n",
      "Epoch [330/1000], Loss: 0.247075\n",
      "Epoch [340/1000], Loss: 0.323244\n",
      "Epoch [350/1000], Loss: 0.227406\n",
      "Epoch [360/1000], Loss: 0.284796\n",
      "Epoch [370/1000], Loss: 0.225994\n",
      "Epoch [380/1000], Loss: 0.211641\n",
      "Epoch [390/1000], Loss: 0.243899\n",
      "Epoch [400/1000], Loss: 0.248536\n",
      "Epoch [410/1000], Loss: 0.225001\n",
      "Epoch [420/1000], Loss: 0.239740\n",
      "Epoch [430/1000], Loss: 0.199403\n",
      "Epoch [440/1000], Loss: 0.214229\n",
      "Epoch [450/1000], Loss: 0.297168\n",
      "Epoch [460/1000], Loss: 0.227665\n",
      "Epoch [470/1000], Loss: 0.271538\n",
      "Epoch [480/1000], Loss: 0.205981\n",
      "Epoch [490/1000], Loss: 0.181826\n",
      "Epoch [500/1000], Loss: 0.185529\n",
      "Epoch [510/1000], Loss: 0.216197\n",
      "Epoch [520/1000], Loss: 0.155786\n",
      "Epoch [530/1000], Loss: 0.138318\n",
      "Epoch [540/1000], Loss: 0.252826\n",
      "Epoch [550/1000], Loss: 0.197281\n",
      "Epoch [560/1000], Loss: 0.179535\n",
      "Epoch [570/1000], Loss: 0.240498\n",
      "Epoch [580/1000], Loss: 0.223373\n",
      "Epoch [590/1000], Loss: 0.219055\n",
      "Epoch [600/1000], Loss: 0.188189\n",
      "Epoch [610/1000], Loss: 0.204232\n",
      "Epoch [620/1000], Loss: 0.237107\n",
      "Epoch [630/1000], Loss: 0.210432\n",
      "Epoch [640/1000], Loss: 0.163764\n",
      "Epoch [650/1000], Loss: 0.220128\n",
      "Epoch [660/1000], Loss: 0.224618\n",
      "Epoch [670/1000], Loss: 0.192122\n",
      "Epoch [680/1000], Loss: 0.205361\n",
      "Epoch [690/1000], Loss: 0.234575\n",
      "Epoch [700/1000], Loss: 0.214384\n",
      "Epoch [710/1000], Loss: 0.166138\n",
      "Epoch [720/1000], Loss: 0.179157\n",
      "Epoch [730/1000], Loss: 0.190557\n",
      "Epoch [740/1000], Loss: 0.173144\n",
      "Epoch [750/1000], Loss: 0.187290\n",
      "Epoch [760/1000], Loss: 0.218200\n",
      "Epoch [770/1000], Loss: 0.163849\n",
      "Epoch [780/1000], Loss: 0.185675\n",
      "Epoch [790/1000], Loss: 0.193648\n",
      "Epoch [800/1000], Loss: 0.196942\n",
      "Epoch [810/1000], Loss: 0.226219\n",
      "Epoch [820/1000], Loss: 0.184163\n",
      "Epoch [830/1000], Loss: 0.188034\n",
      "Epoch [840/1000], Loss: 0.207090\n",
      "Epoch [850/1000], Loss: 0.287285\n",
      "Epoch [860/1000], Loss: 0.183169\n",
      "Epoch [870/1000], Loss: 0.196296\n",
      "Epoch [880/1000], Loss: 0.209126\n",
      "Epoch [890/1000], Loss: 0.189508\n",
      "Epoch [900/1000], Loss: 0.219236\n",
      "Epoch [910/1000], Loss: 0.225898\n",
      "Epoch [920/1000], Loss: 0.196718\n",
      "Epoch [930/1000], Loss: 0.179886\n",
      "Epoch [940/1000], Loss: 0.212743\n",
      "Epoch [950/1000], Loss: 0.208290\n",
      "Epoch [960/1000], Loss: 0.208024\n",
      "Epoch [970/1000], Loss: 0.198316\n",
      "Epoch [980/1000], Loss: 0.199871\n",
      "Epoch [990/1000], Loss: 0.231182\n",
      "Epoch [1000/1000], Loss: 0.181065\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    tot_losses=0\n",
    "    for i in np.random.permutation(input_train.shape[0]//batch_size):\n",
    "        # convert numpy array to torch tensor\n",
    "        input = torch.from_numpy(input_train).float()\n",
    "        output = torch.from_numpy(output_train).float()\n",
    "        loss=0\n",
    "\n",
    "        input_batch = input[i*batch_size:(i+1)*batch_size,:]\n",
    "        output_batch = output[i*batch_size:(i+1)*batch_size,:]\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output_pred = model(input_batch)\n",
    "        # compute loss\n",
    "        loss += F.mse_loss(output_pred, output_batch, reduction='sum')\n",
    "        tot_losses += loss.item()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(tot_losses/(input_train.shape[0]//batch_size))\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {:.6f}'.format(epoch+1, nb_epochs, tot_losses/(input_train.shape[0]//batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.311524\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "# convert numpy array to torch tensor\n",
    "input_test = torch.from_numpy(input_val).float()\n",
    "output_test = torch.from_numpy(output_val).float()\n",
    "predictions= torch.zeros(output_test.shape)\n",
    "tot_losses=0\n",
    "\n",
    "model.eval()\n",
    "# prediction\n",
    "output_pred = model(input_test)\n",
    "# compute loss\n",
    "loss = F.mse_loss(output_pred, output_test, reduction='sum')\n",
    "\n",
    "print('Test loss: {:.6f}'.format(loss*batch_size/input_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1531, 0.1564, 0.1598, 0.1614],\n",
      "        [0.2477, 0.2538, 0.2582, 0.2626],\n",
      "        [0.2980, 0.3049, 0.3104, 0.3164],\n",
      "        [0.2896, 0.2966, 0.3018, 0.3076],\n",
      "        [0.1728, 0.1770, 0.1803, 0.1827]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.1177, 0.1207, 0.1238, 0.1267],\n",
      "        [0.2500, 0.2542, 0.2579, 0.2616],\n",
      "        [0.3858, 0.3905, 0.3950, 0.3996],\n",
      "        [0.2752, 0.2856, 0.2963, 0.3070],\n",
      "        [0.1327, 0.1382, 0.1438, 0.1497]])\n"
     ]
    }
   ],
   "source": [
    "print(output_pred[-5:])\n",
    "print(output_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2126593c4ba056f374a5f953e7ab3e7ee968bf458a40de64ddebe48ce9256529"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
