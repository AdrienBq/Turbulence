{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import netCDF4 as nc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path(sys.path[0]).parent)\n",
    "import modules.utils as utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factors = [32]\n",
    "Directory = f\"data\"\n",
    "\n",
    "variables=['u', 'v', 'w', 'theta', 's', 'tke', 'wtheta']\n",
    "nz=376\n",
    "\n",
    "len_samples = nz*len(variables)\n",
    "len_in = nz*(len(variables)-1)\n",
    "len_out = nz\n",
    "n_in_features = len(variables)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  5 33 54 26  8 17 46 20 60 57 13 31  7 11  3 62  9 56  1 44 32 52 35\n",
      " 22 58 24 47 55 42 23  2 14 27 43 10 21 34 59 45 51  4 41 29 25 18 53 19\n",
      " 28]\n",
      "[12 36 37 50 40 16 48 38  6 39 15 61 30]\n"
     ]
    }
   ],
   "source": [
    "model_number = 11\n",
    "\n",
    "train_times = pd.read_csv(Directory+f'/test_train_times/times_train_{model_number}.csv').drop(columns=['Unnamed: 0']).to_numpy().transpose()[0]\n",
    "test_times = pd.read_csv(Directory+f'/test_train_times/times_test_{model_number}.csv').drop(columns=['Unnamed: 0']).to_numpy().transpose()[0]\n",
    "print(train_times)\n",
    "print(test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_train, output_train, input_test, output_test = utils.make_train_test_ds(coarse_factors, len_in, train_times, test_times, Directory)\n",
    "ins = [input_train, input_test]\n",
    "outs = [output_train, output_test]\n",
    "\n",
    "for j in range(len(ins)):\n",
    "    input = ins[j]\n",
    "    input = input.reshape(-1,len(variables)-1,nz)\n",
    "    for i in range(len(variables)-1):\n",
    "        input[:,i] -= torch.mean(input[:,i])\n",
    "        input[:,i] /= torch.std(input[:,i])\n",
    "    ins[j] = input\n",
    "\n",
    "for i in range(len(outs)):\n",
    "    output = outs[i]\n",
    "    output -= torch.mean(output)\n",
    "    output /= torch.std(output)\n",
    "    outs[i] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_features, output_features, drop_prob1=0.2, drop_prob2=0.3, drop_prob3=0.4, hidden_size1=128, hidden_size2=256, hidden_size3=256):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_features, out_channels=input_features, kernel_size=2, stride=1, padding=0, dilation=1, groups=input_features, bias=True)\n",
    "        self.conv2 = nn.Conv1d(in_channels=input_features, out_channels=input_features, kernel_size=3, stride=1, padding=1, dilation=1, groups=input_features, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(input_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm1d(input_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.regression = nn.Sequential(nn.BatchNorm1d(int(input_features*(nz-1)/(3*5)), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Linear(int(input_features*(nz-1)/(3*5)), hidden_size1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob1),\n",
    "                                        nn.Linear(hidden_size1, hidden_size2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob2),\n",
    "                                        nn.Linear(hidden_size2, hidden_size3),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob3),\n",
    "                                        nn.Linear(hidden_size3, output_features))\n",
    "\n",
    "        self.drop_prob1 = drop_prob1\n",
    "        self.drop_prob2 = drop_prob2\n",
    "        self.drop_prob3 = drop_prob3\n",
    "        self.input_shape = int(input_features*(nz-1)/(3*5))\n",
    "        self.output_shape = nz\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "                                        \n",
    "\n",
    "    def forward(self, x):       # x is of shape (batch_size, input_features, nz), in_size = nz*input_features\n",
    "        x = F.max_pool1d(input=self.conv1(self.bn1(x)), kernel_size=5)\n",
    "        x = F.max_pool1d(input=self.conv2(self.bn2(x)), kernel_size=3)\n",
    "        x = torch.flatten(x, start_dim=1,end_dim=-1)\n",
    "        return self.regression(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, input_test, output_test):\n",
    "    model.eval()\n",
    "    # prediction\n",
    "    output_pred = model(input_test.to(device))\n",
    "    # compute loss\n",
    "    test_loss = F.mse_loss(output_pred, output_test.to(device), reduction='mean')\n",
    "    return test_loss.item()\n",
    "\n",
    "def train(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, input_train, output_train, input_test, output_test, len_in, len_out):\n",
    "    for learning_rate in learning_rates:\n",
    "        train_losses_lr = []\n",
    "        test_losses_lr = []\n",
    "        for decay in decays:\n",
    "            train_losses_decay = []\n",
    "            test_losses_decay = []\n",
    "            for batch_size in batch_sizes :\n",
    "                n_batches = input_train.shape[0]//batch_size\n",
    "                model = CNN(input_features=len_in,output_features=len_out)\n",
    "                model = model.to(device)\n",
    "                print(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, decay, last_epoch= -1)\n",
    "                models.append(model)\n",
    "                train_losses_bs = []\n",
    "                test_losses_bs = []\n",
    "                for epoch in trange(nb_epochs[0], leave=False):\n",
    "                    model.train()\n",
    "                    tot_losses=0\n",
    "                    indexes_arr = np.random.permutation(input_train.shape[0]).reshape(-1, batch_size)\n",
    "                    for i_batch in indexes_arr:\n",
    "                        input_batch = input_train[i_batch,:,:].to(device)\n",
    "                        output_batch = output_train[i_batch,:].to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        # forward pass\n",
    "                        #print('input_batch device : ', input_batch.get_device())\n",
    "                        #print('output_batch device : ', output_batch.get_device())\n",
    "                        output_pred = model(input_batch)\n",
    "                        # compute loss\n",
    "                        #print(\"out size :\", output_pred.shape, \", out batch size :\", output_batch.shape)\n",
    "                        loss = F.mse_loss(output_pred, output_batch, reduction='mean')\n",
    "                        tot_losses += loss.item()\n",
    "                        # backward pass\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    train_losses_bs.append(tot_losses/n_batches)     # loss moyenne sur tous les batchs \n",
    "                    #print(tot_losses)                               # comme on a des batch 2 fois plus petit (16 au lieu de 32)\n",
    "                                                                    # on a une loss en moyenne 2 fois plus petite\n",
    "\n",
    "                    test_losses_bs.append(test(model, device, input_test, output_test))\n",
    "\n",
    "                    if epoch < 300:\n",
    "                        scheduler.step()\n",
    "\n",
    "                print('Model {},{},{},Epoch [{}/{}], Loss: {:.6f}'.format(learning_rate, decay, batch_size, epoch+1, nb_epochs[0], tot_losses/n_batches))\n",
    "                train_losses_decay.append(train_losses_bs)\n",
    "                test_losses_decay.append(test_losses_bs)\n",
    "            train_losses_lr.append(train_losses_decay)\n",
    "            test_losses_lr.append(test_losses_decay)\n",
    "        train_losses.append(train_losses_lr)\n",
    "        test_losses.append(test_losses_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000009?line=5'>6</a>\u001b[0m test_losses\u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000009?line=6'>7</a>\u001b[0m models\u001b[39m=\u001b[39m[]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000009?line=8'>9</a>\u001b[0m train(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, ins[\u001b[39m0\u001b[39;49m], outs[\u001b[39m0\u001b[39;49m], ins[\u001b[39m1\u001b[39;49m], outs[\u001b[39m1\u001b[39;49m], n_in_features, nz)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000009?line=9'>10</a>\u001b[0m train_losses_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_losses)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000009?line=10'>11</a>\u001b[0m test_losses_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test_losses)\n",
      "\u001b[1;32m/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, input_train, output_train, input_test, output_test, len_in, len_out)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=44'>45</a>\u001b[0m train_losses_bs\u001b[39m.\u001b[39mappend(tot_losses\u001b[39m/\u001b[39mn_batches)     \u001b[39m# loss moyenne sur tous les batchs \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=45'>46</a>\u001b[0m \u001b[39m#print(tot_losses)                               # comme on a des batch 2 fois plus petit (16 au lieu de 32)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=46'>47</a>\u001b[0m                                                 \u001b[39m# on a une loss en moyenne 2 fois plus petite\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=48'>49</a>\u001b[0m test_losses_bs\u001b[39m.\u001b[39mappend(test(model, device, input_test, output_test))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=51'>52</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb Cell 8'\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, input_test, output_test)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39m# prediction\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=3'>4</a>\u001b[0m output_pred \u001b[39m=\u001b[39m model(input_test\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=4'>5</a>\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000007?line=5'>6</a>\u001b[0m test_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(output_pred, output_test\u001b[39m.\u001b[39mto(device), reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb Cell 7'\u001b[0m in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000006?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):       \u001b[39m# x is of shape (batch_size, input_features, nz), in_size = nz*input_features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000006?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool1d(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)), kernel_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000006?line=34'>35</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool1d(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)), kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adrien/Documents/Cours_Polytechnique_3A/stage/Turbulence/explo/conv_net.ipynb#ch0000006?line=35'>36</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,end_dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=300'>301</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=301'>302</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=293'>294</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=294'>295</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=295'>296</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=296'>297</a>\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=297'>298</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py?line=298'>299</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [3*1e-3]\n",
    "decays = [0.95]\n",
    "batch_sizes = [32]             # obligé de le mettre à 16 si pls L car sinon le nombre total de samples n'est pas divisible par batch_size \n",
    "nb_epochs = [10]               # et on ne peut donc pas reshape. Sinon il ne pas prendre certains samples pour que ça tombe juste.\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "models=[]\n",
    "\n",
    "train(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, ins[0], outs[0], ins[1], outs[1], n_in_features, nz)\n",
    "train_losses_arr = np.array(train_losses)\n",
    "test_losses_arr = np.array(test_losses)\n",
    "print(\"train losses array shape : \", train_losses_arr.shape)\n",
    "print(\"test losses array shape : \", test_losses_arr.shape)\n",
    "\n",
    "#for i in range(len(models)):\n",
    "    #torch.save(models[i].state_dict(), f\"explo/models/pca_{i}.pt\")\n",
    "\n",
    "fig,axes = plt.subplots(len(decays),len(batch_sizes)*len(learning_rates),figsize=(5*len(decays),4*len(batch_sizes)*len(learning_rates)))\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(decays)):\n",
    "        for k in range(len(batch_sizes)):\n",
    "            try : \n",
    "                axes[j,k+i*len(batch_sizes)].plot(train_losses_arr[i,j,k,1:], label='train')\n",
    "                axes[j,k+i*len(batch_sizes)].plot(test_losses_arr[i,j,k,1:], label='test')\n",
    "                axes[j,k+i*len(batch_sizes)].set_title(f\"d = {decays[j]}, lr = {learning_rates[i]}, bs = {batch_sizes[k]}\")\n",
    "                axes[j,k+i*len(batch_sizes)].legend()\n",
    "            except :\n",
    "                pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
