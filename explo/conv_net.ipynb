{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import netCDF4 as nc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path(sys.path[0]).parent)\n",
    "import modules.utils as utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factors = [32]\n",
    "Directory = f\"data\"\n",
    "\n",
    "variables=['u', 'v', 'w', 'theta', 's', 'tke', 'wtheta']\n",
    "nz=376\n",
    "\n",
    "len_samples = nz*len(variables)\n",
    "len_in = nz*(len(variables)-1)\n",
    "len_out = nz\n",
    "n_in_features = len(variables)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  5 33 54 26  8 17 46 20 60 57 13 31  7 11  3 62  9 56  1 44 32 52 35\n",
      " 22 58 24 47 55 42 23  2 14 27 43 10 21 34 59 45 51  4 41 29 25 18 53 19\n",
      " 28]\n",
      "[12 36 37 50 40 16 48 38  6 39 15 61 30]\n"
     ]
    }
   ],
   "source": [
    "model_number = 11\n",
    "\n",
    "train_times = pd.read_csv(Directory+f'/test_train_times/times_train_{model_number}.csv').drop(columns=['Unnamed: 0']).to_numpy().transpose()[0]\n",
    "test_times = pd.read_csv(Directory+f'/test_train_times/times_test_{model_number}.csv').drop(columns=['Unnamed: 0']).to_numpy().transpose()[0]\n",
    "print(train_times)\n",
    "print(test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_train, output_train, input_test, output_test = utils.make_train_test_ds(coarse_factors, len_in, train_times, test_times, Directory)\n",
    "ins = [input_train, input_test]\n",
    "outs = [output_train, output_test]\n",
    "\n",
    "for j in range(len(ins)):\n",
    "    input = ins[j]\n",
    "    input = input.reshape(-1,len(variables)-1,nz)\n",
    "    for i in range(len(variables)-1):\n",
    "        input[:,i] -= torch.mean(input[:,i])\n",
    "        input[:,i] /= torch.std(input[:,i])\n",
    "    ins[j] = input\n",
    "\n",
    "for i in range(len(outs)):\n",
    "    output = outs[i]\n",
    "    output -= torch.mean(output)\n",
    "    output /= torch.std(output)\n",
    "    outs[i] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_features, output_features, drop_prob1=0.2, drop_prob2=0.3, drop_prob3=0.4, hidden_size1=128, hidden_size2=256, hidden_size3=256):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_features, out_channels=input_features, kernel_size=2, stride=1, padding=0, dilation=1, groups=input_features, bias=True)\n",
    "        self.conv2 = nn.Conv1d(in_channels=input_features, out_channels=input_features, kernel_size=3, stride=1, padding=1, dilation=1, groups=input_features, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(input_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm1d(input_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.regression = nn.Sequential(nn.BatchNorm1d(int(input_features*(nz-1)/(3*5)), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Linear(int(input_features*(nz-1)/(3*5)), hidden_size1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob1),\n",
    "                                        nn.Linear(hidden_size1, hidden_size2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob2),\n",
    "                                        nn.Linear(hidden_size2, hidden_size3),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm1d(hidden_size3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                                        nn.Dropout(drop_prob3),\n",
    "                                        nn.Linear(hidden_size3, output_features))\n",
    "\n",
    "        self.drop_prob1 = drop_prob1\n",
    "        self.drop_prob2 = drop_prob2\n",
    "        self.drop_prob3 = drop_prob3\n",
    "        self.input_shape = int(input_features*(nz-1)/(3*5))\n",
    "        self.output_shape = nz\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "                                        \n",
    "\n",
    "    def forward(self, x):       # x is of shape (batch_size, input_features, nz), in_size = nz*input_features\n",
    "        x = F.max_pool1d(input=self.conv1(self.bn1(x)), kernel_size=5)\n",
    "        x = F.max_pool1d(input=self.conv2(self.bn2(x)), kernel_size=3)\n",
    "        x = x.reshape(-1, x.shape[-2]*x.shape[-1])\n",
    "        return self.regression(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, input_test, output_test):\n",
    "    model.eval()\n",
    "    # prediction\n",
    "    output_pred = model(input_test.to(device))\n",
    "    # compute loss\n",
    "    test_loss = F.mse_loss(output_pred, output_test.to(device), reduction='mean')\n",
    "    return test_loss.item()\n",
    "\n",
    "def train(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, input_train, output_train, input_test, output_test, len_in, len_out):\n",
    "    for learning_rate in learning_rates:\n",
    "        train_losses_lr = []\n",
    "        test_losses_lr = []\n",
    "        for decay in decays:\n",
    "            train_losses_decay = []\n",
    "            test_losses_decay = []\n",
    "            for batch_size in batch_sizes :\n",
    "                n_batches = input_train.shape[0]//batch_size\n",
    "                model = CNN(input_features=len_in,output_features=len_out)\n",
    "                model = model.to(device)\n",
    "                print(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, decay, last_epoch= -1)\n",
    "                models.append(model)\n",
    "                train_losses_bs = []\n",
    "                test_losses_bs = []\n",
    "                for epoch in trange(nb_epochs[0], leave=False):\n",
    "                    model.train()\n",
    "                    tot_losses=0\n",
    "                    indexes_arr = np.random.permutation(input_train.shape[0]).reshape(-1, batch_size)\n",
    "                    for i_batch in indexes_arr:\n",
    "                        input_batch = input_train[i_batch,:,:].to(device)\n",
    "                        output_batch = output_train[i_batch,:].to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        # forward pass\n",
    "                        #print('input_batch device : ', input_batch.get_device())\n",
    "                        #print('output_batch device : ', output_batch.get_device())\n",
    "                        output_pred = model(input_batch)\n",
    "                        # compute loss\n",
    "                        #print(\"out size :\", output_pred.shape, \", out batch size :\", output_batch.shape)\n",
    "                        loss = F.mse_loss(output_pred, output_batch, reduction='mean')\n",
    "                        tot_losses += loss.item()\n",
    "                        # backward pass\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    train_losses_bs.append(tot_losses/n_batches)     # loss moyenne sur tous les batchs \n",
    "                    #print(tot_losses)                               # comme on a des batch 2 fois plus petit (16 au lieu de 32)\n",
    "                                                                    # on a une loss en moyenne 2 fois plus petite\n",
    "\n",
    "                    test_losses_bs.append(test(model, device, input_test, output_test))\n",
    "\n",
    "                    if epoch < 300:\n",
    "                        scheduler.step()\n",
    "\n",
    "                print('Model {},{},{},Epoch [{}/{}], Loss: {:.6f}'.format(learning_rate, decay, batch_size, epoch+1, nb_epochs[0], tot_losses/n_batches))\n",
    "                train_losses_decay.append(train_losses_bs)\n",
    "                test_losses_decay.append(test_losses_bs)\n",
    "            train_losses_lr.append(train_losses_decay)\n",
    "            test_losses_lr.append(test_losses_decay)\n",
    "        train_losses.append(train_losses_lr)\n",
    "        test_losses.append(test_losses_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0.003,0.95,32,Epoch [2/2], Loss: 0.218664\n",
      "train losses array shape :  (1, 1, 1, 2)\n",
      "test losses array shape :  (1, 1, 1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [3*1e-3]\n",
    "decays = [0.95]\n",
    "batch_sizes = [32]             # obligé de le mettre à 16 si pls L car sinon le nombre total de samples n'est pas divisible par batch_size \n",
    "nb_epochs = [25]               # et on ne peut donc pas reshape. Sinon il ne pas prendre certains samples pour que ça tombe juste.\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "models=[]\n",
    "\n",
    "train(device, learning_rates, decays, batch_sizes, nb_epochs, models, train_losses, test_losses, ins[0], outs[0], ins[1], outs[1], n_in_features, nz)\n",
    "train_losses_arr = np.array(train_losses)\n",
    "test_losses_arr = np.array(test_losses)\n",
    "print(\"train losses array shape : \", train_losses_arr.shape)\n",
    "print(\"test losses array shape : \", test_losses_arr.shape)\n",
    "\n",
    "#for i in range(len(models)):\n",
    "    #torch.save(models[i].state_dict(), f\"explo/models/pca_{i}.pt\")\n",
    "\n",
    "fig,axes = plt.subplots(len(decays),len(batch_sizes)*len(learning_rates),figsize=(5*len(decays),4*len(batch_sizes)*len(learning_rates)))\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(decays)):\n",
    "        for k in range(len(batch_sizes)):\n",
    "            try : \n",
    "                axes[j,k+i*len(batch_sizes)].plot(train_losses_arr[i,j,k,1:], label='train')\n",
    "                axes[j,k+i*len(batch_sizes)].plot(test_losses_arr[i,j,k,1:], label='test')\n",
    "                axes[j,k+i*len(batch_sizes)].set_title(f\"d = {decays[j]}, lr = {learning_rates[i]}, bs = {batch_sizes[k]}\")\n",
    "                axes[j,k+i*len(batch_sizes)].legend()\n",
    "            except :\n",
    "                pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
